[global_tags]
  country_code = "${TAG_COUNTRY_CODE}"
  facility_id = "${TAG_FACILITY_ID}"
  rack_id = "${TAG_RACK_ID}"
  server_id = "${TAG_SERVER_ID}"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "30s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Collection offset is used to shift the collection by the given amount.
  ## This can be be used to avoid many plugins querying constraint devices
  ## at the same time by manually scheduling them in time.
  # collection_offset = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## Collected metrics are rounded to the precision specified. Precision is
  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  ##
  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s:
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ##
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  precision = "0s"

  ## Log at debug level.
  # debug = false
  ## Log only error level messages.
  # quiet = false

  ## Log format controls the way messages are logged and can be one of "text",
  ## "structured" or, on Windows, "eventlog".
  # logformat = "text"

  ## Message key for structured logs, to override the default of "msg".
  ## Ignored if `logformat` is not "structured".
  # structured_log_message_key = "message"

  ## Name of the file to be logged to or stderr if unset or empty. This
  ## setting is ignored for the "eventlog" format.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0h"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  # log_with_timezone = ""

  ## Override default hostname, if empty use os.Hostname()
  # hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  # omit_hostname = false

  ## Method of translating SNMP objects. Can be "netsnmp" (deprecated) which
  ## translates by calling external programs snmptranslate and snmptable,
  ## or "gosmi" which translates using the built-in gosmi library.
  # snmp_translator = "netsnmp"

  ## Name of the file to load the state of plugins from and store the state to.
  ## If uncommented and not empty, this file will be used to save the state of
  ## stateful plugins on termination of Telegraf. If the file exists on start,
  ## the state in the file will be restored for the plugins.
  # statefile = ""

  ## Flag to skip running processors after aggregators
  ## By default, processors are run a second time after aggregators. Changing
  ## this setting to true will skip the second run of processors.
  # skip_processors_after_aggregators = false

#[[outputs.influxdb_v2]]
#   urls = ["${OUTPUT_INFLUXDB_URL}"]
#   token = "${OUTPUT_INFLUXDB_TOKEN}"
#   organization = "${OUTPUT_INFLUXDB_ORGANIZATION}"
#   bucket = "${TAG_FACILITY_ID}" # registrar creates a bucket for each facility

[[inputs.ipmi_sensor]]
  sensors = ["sdr", "chassis_power_status", "dcmi_power_reading"]
  fielddrop = ["status"]
  [inputs.ipmi_sensor.tagpass]
    name = ["average_power_reading_over_sample_period", "fan*"]

[[processors.rename]]
  alias = "rename_ipmi_power_metrics"
  namepass = ["ipmi_sensor"]
  [[processors.rename.replace]]
    measurement = "ipmi_sensor"
    dest = "server"

  [[processors.rename.replace]]
    field = "value"
    dest = "server_energy_consumption_watts"

  [processors.rename.tagpass]
    name = ["average_power_reading_over_sample_period"]

[[processors.rename]]
  alias = "rename_ipmi_fan_metrics"
  namepass = ["ipmi_sensor"]
  [[processors.rename.replace]]
    measurement = "ipmi_sensor"
    dest = "server"

  [[processors.rename.replace]]
    field = "value"
    dest = "server_fan_speed_rpm"

  [[processors.rename.replace]]
    tag = "name"
    dest = "fan"

  [processors.rename.tagpass]
    name = ["fan*"]

[[inputs.net]]
  interfaces = ["eno*"]
  fieldpass = ["bytes_sent", "bytes_recv", "packets_sent", "packets_recv"]
  [inputs.net.tagdrop]
    interface = ["all"]

[[processors.starlark]]
  alias = "calculate network rates"
  namepass = ["net"]
  script = "diff.star"
  [processors.starlark.constants]
    key_tag = "interface"

[[processors.rename]]
  alias = "rename_net_metrics"
  namepass = ["net"]
  [[processors.rename.replace]]
    measurement = "net"
    dest = "server"

  [[processors.rename.replace]]
    field = "bytes_sent"
    dest = "network_transmit_bytes"

  [[processors.rename.replace]]
    field = "bytes_recv"
    dest = "network_received_bytes"

  [[processors.rename.replace]]
    field = "packets_sent"
    dest = "network_transmit_packets"

  [[processors.rename.replace]]
    field = "packets_recv"
    dest = "network_received_packets"

  [[processors.rename.replace]]
    tag = "interface"
    dest = "device"

[[inputs.diskio]]
  devices = ["sda", "sdb"]
  fieldpass = ["reads", "writes", "read_bytes", "write_bytes"]

[[processors.starlark]]
  alias = "calculate disk io rates"
  namepass = ["diskio"]
  script = "diff.star"
  [processors.starlark.constants]
    key_tag = "name"

[[processors.rename]]
  alias = "rename_diskio_metrics"
  namepass = ["diskio"]
  [[processors.rename.replace]]
    measurement = "diskio"
    dest = "server"

  [[processors.rename.replace]]
    field = "writes"
    dest = "io_writes"

  [[processors.rename.replace]]
    field = "reads"
    dest = "io_reads"

  [[processors.rename.replace]]
    field = "write_bytes"
    dest = "io_bytes_written"

  [[processors.rename.replace]]
    field = "read_bytes"
    dest = "io_bytes_read"

  [[processors.rename.replace]]
    tag = "name"
    dest = "device"

#[[inputs.intel_powerstat]]
# # Intel PowerStat plugin enables monitoring of platform metrics (power, TDP)
# # and per-CPU metrics like temperature, power and utilization. Please see the
# # plugin readme for details on software and hardware compatibility.
# # This plugin ONLY supports Linux.
# [[inputs.intel_powerstat]]
#   ## The user can choose which package metrics are monitored by the plugin with
#   ## the package_metrics setting:
#   ## - The default, will collect "current_power_consumption",
#   ##   "current_dram_power_consumption" and "thermal_design_power".
#   ## - Leaving this setting empty means no package metrics will be collected.
#   ## - Finally, a user can specify individual metrics to capture from the
#   ##   supported options list.
#   ## Supported options:
#   ##   "current_power_consumption", "current_dram_power_consumption",
#   ##   "thermal_design_power", "max_turbo_frequency", "uncore_frequency",
#   ##   "cpu_base_frequency"
#   # package_metrics = ["current_power_consumption", "current_dram_power_consumption", "thermal_design_power"]
#
#   ## The user can choose which per-CPU metrics are monitored by the plugin in
#   ## cpu_metrics array.
#   ## Empty or missing array means no per-CPU specific metrics will be collected
#   ## by the plugin.
#   ## Supported options:
#   ##   "cpu_frequency", "cpu_c0_state_residency", "cpu_c1_state_residency",
#   ##   "cpu_c3_state_residency", "cpu_c6_state_residency", "cpu_c7_state_residency",
#   ##   "cpu_temperature", "cpu_busy_frequency", "cpu_c0_substate_c01",
#   ##   "cpu_c0_substate_c02", "cpu_c0_substate_c0_wait"
#   # cpu_metrics = []
#
#   ## CPUs metrics to include from those configured in cpu_metrics array
#   ## Can't be combined with excluded_cpus. Empty means all CPUs are gathered.
#   ## e.g. ["0-3", "4,5,6"] or ["1-3,4"]
#   # included_cpus = []
#
#   ## CPUs metrics to exclude from those configured in cpu_metrics array
#   ## Can't be combined with included_cpus. Empty means all CPUs are gathered.
#   ## e.g. ["0-3", "4,5,6"] or ["1-3,4"]
#   # excluded_cpus = []
#
#   ## Filesystem location of JSON file that contains PMU event definitions.
#   ## Mandatory only for perf-related metrics (cpu_c0_substate_c01, cpu_c0_substate_c02, cpu_c0_substate_c0_wait).
#   # event_definitions = ""
#
#   ## The user can set the timeout duration for MSR reading.
#   ## Enabling this timeout can be useful in situations where, on heavily loaded systems,
#   ## the code waits too long for a kernel response to MSR read requests.
#   ## 0 disables the timeout (default).
#   # msr_read_timeout = "0ms"


#[[inputs.nvidia_smi]]
# [[inputs.nvidia_smi]]
#   ## Optional: path to nvidia-smi binary, defaults "/usr/bin/nvidia-smi"
#   ## We will first try to locate the nvidia-smi binary with the explicitly specified value (or default value),
#   ## if it is not found, we will try to locate it on PATH(exec.LookPath), if it is still not found, an error will be returned
#   # bin_path = "/usr/bin/nvidia-smi"
#
#   ## Optional: timeout for GPU polling
#   # timeout = "5s"

# This processor calculates the server metrics from the spec (https://github.com/SDIAlliance/nadiki-api/blob/main/server/server-api.spec.yaml)
# from those that the above input plugins produce.
# We need to call python with "-u" in order to deactivate output buffering
#[[processors.execd]]
#  command = ["python3", "-u", "nadiki-server-telegraf-processor.py"]
#  namepass = ["nvidia_smi", "powerstat_package", "ipmi_sensor"] # this processor will swallow all its input, so we limit it to the metrics it needs
#
#
[[inputs.execd]]
  alias = "nadiki-facility-zabbix-crawler"
  command = ["python3", "-u", "nadiki-facility-zabbix-crawler.py"]
  data_format = "influx"
  signal = "SIGHUP"

[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = false
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  ## NOTE: The resulting 'time_active' field INCLUDES 'iowait'!
  report_active = true
  ## If true and the info is available then add core_id and physical_id tags
  core_tags = false
  fieldinclude = ["usage_active"]

[[processors.scale]]
  alias = "scale cpu metrics to [0..1]"
  [[processors.scale.scaling]]
    metricpass = ["cpu"]
    factor = 0.01
    fields = ["usage_active"]

[[processors.rename]]
  alias = "rename cpu metrics"
  metricpass = ["cpu"]
  ## Specify one sub-table per rename operation.
  [[processors.rename.replace]]
    measurement = "cpu"
    dest = "server"

  [[processors.rename.replace]]
    field = "usage_active"
    dest = "cpu_busy_fraction"

#
#[[inputs.exec]]
#  commands = ["python3 -u nadiki-facility-electricitymap-crawler.py"]
#  data_format = "influx"
#

#[[processors.starlark]]
#  source = '''
#def apply(metric):
#  last_metric = state.get("last_metric")
#  state["last_metric"] = metric
#  if last_metric == None:
#    return None
#  else:
#    result = Metric("difference")
#    for f in metric.fields.keys():
#        result.fields[f] = metric.fields[f] - last_metric.fields[f]
#    return result
#'''

#[[aggregators.basicstats]]
#  drop_original = true
#  stats = ["count", "non_negative_diff"]
#  period = "60s"

[[outputs.file]]
  files = ["stdout"]

